{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Character Level Language Model \n",
    "## Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import numpy as np\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for the model\n",
    "batch_size = 64  # batch size \n",
    "num_epochs = 15  # total epochs to train\n",
    "latent_dim = 256  # no. of activation units or latent dimensionality of encoder\n",
    "num_samples = 10000 # no. of samples to train on\n",
    "\n",
    "# path to look for the data file \n",
    "data_path = r'data/hin-eng/hin.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []  # for storing the input text data\n",
    "target_texts = []  # for storing the target text data\n",
    "input_chars = []  # for storing the unique chars in input text data\n",
    "target_chars = []  # for storing the unique chars in target text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Initialization\n",
    "encoder_unique_tokens = 0 # unique tokens in encoder input \n",
    "decoder_unique_tokens = 0 # unique tokens in decoder output\n",
    "Tx = 0 # max length of input sequence for encoder\n",
    "Ty = 0 # max length of output sequence for decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of lines of Original Text data: 149862\n",
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 94\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for Target outputs: 59\n"
     ]
    }
   ],
   "source": [
    "# read the data file\n",
    "with open(data_path, 'r', encoding = 'utf-8') as file:\n",
    "    text_data = file.read().split('\\n')\n",
    "    \n",
    "print('Total no. of lines of Original Text data: ' + str(len(text_data)))\n",
    "\n",
    "# add input and target data\n",
    "end_index = min(num_samples, len(text_data) - 1)\n",
    "for line in text_data[: end_index]:\n",
    "    # since each line is of format: Lang1 + '\\t' + Lang2\n",
    "    input_line, target_line = line.split('\\t')\n",
    "    \n",
    "    # we will use '\\t' as start_char and '\\n' as end character\n",
    "    target_line = '\\t' + str(target_line) + '\\n'\n",
    "    input_texts.append(input_line)\n",
    "    target_texts.append(target_line)\n",
    "    \n",
    "    # update the max. sequence lengths for encoder and decoder\n",
    "    Tx = max(Tx, len(input_line))\n",
    "    Ty = max(Ty, len(target_line))\n",
    "    \n",
    "    # find the unique characters in input and target text data\n",
    "    for char in input_line:\n",
    "        if char not in input_chars:\n",
    "            input_chars.append(char)\n",
    "    for char in target_line:\n",
    "        if char not in target_chars:\n",
    "            target_chars.append(char)\n",
    "            \n",
    "encoder_unique_tokens = len(input_chars)\n",
    "decoder_unique_tokens = len(target_chars)\n",
    "input_chars = sorted(input_chars)\n",
    "target_chars = sorted(target_chars)\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', encoder_unique_tokens)\n",
    "print('Number of unique output tokens:', decoder_unique_tokens)\n",
    "print('Max sequence length for inputs:', Tx)\n",
    "print('Max sequence length for Target outputs:', Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '$', '&', \"'\", ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '’']\n",
      "\n",
      "\n",
      "\n",
      "['\\t', '\\n', ' ', '!', '$', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '3', '5', '6', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '«', '»', 'À', 'Ç', 'É', 'Ê', 'à', 'â', 'ç', 'è', 'é', 'ê', 'ë', 'î', 'ï', 'ô', 'ù', 'û', 'œ', '\\u2009', '‘', '’', '\\u202f']\n"
     ]
    }
   ],
   "source": [
    "print(input_chars)\n",
    "print('\\n\\n')\n",
    "print(target_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of keys in input data char to idx dict: 71\n",
      "{' ': 0, '!': 1, '$': 2, '&': 3, \"'\": 4, ',': 5, '-': 6, '.': 7, '0': 8, '1': 9, '2': 10, '3': 11, '4': 12, '5': 13, '6': 14, '7': 15, '8': 16, '9': 17, ':': 18, '?': 19, 'A': 20, 'B': 21, 'C': 22, 'D': 23, 'E': 24, 'F': 25, 'G': 26, 'H': 27, 'I': 28, 'J': 29, 'K': 30, 'L': 31, 'M': 32, 'N': 33, 'O': 34, 'P': 35, 'Q': 36, 'R': 37, 'S': 38, 'T': 39, 'U': 40, 'V': 41, 'W': 42, 'Y': 43, 'a': 44, 'b': 45, 'c': 46, 'd': 47, 'e': 48, 'f': 49, 'g': 50, 'h': 51, 'i': 52, 'j': 53, 'k': 54, 'l': 55, 'm': 56, 'n': 57, 'o': 58, 'p': 59, 'q': 60, 'r': 61, 's': 62, 't': 63, 'u': 64, 'v': 65, 'w': 66, 'x': 67, 'y': 68, 'z': 69, '’': 70}\n",
      "\n",
      "\n",
      "\n",
      "No. of keys in target data char to idx dict: 94\n",
      "{'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '$': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, ',': 9, '-': 10, '.': 11, '0': 12, '1': 13, '3': 14, '5': 15, '6': 16, '8': 17, '9': 18, ':': 19, '?': 20, 'A': 21, 'B': 22, 'C': 23, 'D': 24, 'E': 25, 'F': 26, 'G': 27, 'H': 28, 'I': 29, 'J': 30, 'K': 31, 'L': 32, 'M': 33, 'N': 34, 'O': 35, 'P': 36, 'Q': 37, 'R': 38, 'S': 39, 'T': 40, 'U': 41, 'V': 42, 'Y': 43, 'a': 44, 'b': 45, 'c': 46, 'd': 47, 'e': 48, 'f': 49, 'g': 50, 'h': 51, 'i': 52, 'j': 53, 'k': 54, 'l': 55, 'm': 56, 'n': 57, 'o': 58, 'p': 59, 'q': 60, 'r': 61, 's': 62, 't': 63, 'u': 64, 'v': 65, 'w': 66, 'x': 67, 'y': 68, 'z': 69, '\\xa0': 70, '«': 71, '»': 72, 'À': 73, 'Ç': 74, 'É': 75, 'Ê': 76, 'à': 77, 'â': 78, 'ç': 79, 'è': 80, 'é': 81, 'ê': 82, 'ë': 83, 'î': 84, 'ï': 85, 'ô': 86, 'ù': 87, 'û': 88, 'œ': 89, '\\u2009': 90, '‘': 91, '’': 92, '\\u202f': 93}\n",
      "\n",
      "\n",
      "\n",
      "No. of keys in input data idx to char dict: 71\n",
      "No. of keys in target data idx to char dict: 94\n"
     ]
    }
   ],
   "source": [
    "# we feed numerical values to a RNN so for that we need to convert\n",
    "# the chars to numbers, so we make a mapping of chars to numbers\n",
    "input_char_idx = dict( [(char, i) for i, char in enumerate(input_chars)] )\n",
    "target_char_idx = dict( [(char, i) for i, char in enumerate(target_chars)] )\n",
    "\n",
    "# dict for reverse lookup from indices to tokens\n",
    "input_idx_char = dict( (i, char) for char, i in input_char_idx.items() ) \n",
    "target_idx_char = dict( (i, char) for char, i in target_char_idx.items() )\n",
    "\n",
    "print('No. of keys in input data char to idx dict: ' + str(len(input_char_idx)))\n",
    "print(input_char_idx)\n",
    "print('\\n\\n')\n",
    "print('No. of keys in target data char to idx dict: ' + str(len(target_char_idx)))\n",
    "print(target_char_idx)\n",
    "\n",
    "print('\\n\\n')\n",
    "print('No. of keys in input data idx to char dict: ' + str(len(input_idx_char)))\n",
    "print('No. of keys in target data idx to char dict: ' + str(len(target_idx_char)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will prepare data for the model\n",
    "# no. of training examples\n",
    "m = len(input_texts)\n",
    "# encoder input data\n",
    "enc_input_data = np.zeros((m, Tx, encoder_unique_tokens), dtype = 'float32')\n",
    "# decoder input data\n",
    "dec_input_data = np.zeros((m, Ty, decoder_unique_tokens), dtype = 'float32')\n",
    "# decoder output target data\n",
    "dec_target_data = np.zeros((m, Ty, decoder_unique_tokens), dtype = 'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training examples\n",
    "Training examples will be of format (X, Y), where X is input and Y is target output.\n",
    "\n",
    "For X we will take text sentences from **input_texts** and for Y we will take text sentences from **target_texts**.<br>\n",
    "But for machine translation we will be using an Architecture where the output from the encoder network is given to the decoder network and using that it produces the target output in the 1st time step , then that produced output is again fed to the decoder network in the next time step, this continues till we get **'\\n'** or exceed max sequence length. \n",
    "\n",
    "For the decoder network the output in each time step is one time step ahead of the input. The 1st input is **start_char** to the decoder and the output for that time step is the input for the next time step.\n",
    "\n",
    "For input we will be using **One Hot encoding(OHE)** for the encoder network. Similarly for the decoder network we will be using **OHE** for input and output representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training examples\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    # for encoder network\n",
    "    # make the indices for the chars hot i.e, 1 in input text\n",
    "    for curr_timestep, char in enumerate(input_text):\n",
    "        enc_input_data[i, curr_timestep, input_char_idx[char]] = 1\n",
    "        \n",
    "    # for decoder network\n",
    "    # make the indices for the chars hot i.e, 1 in target input text\n",
    "    for curr_timestep, char in enumerate(target_text):\n",
    "        dec_input_data[i, curr_timestep, target_char_idx[char]] = 1\n",
    "    if curr_timestep > 0:     \n",
    "        # make the indices for the chars hot i.e, 1 in target text, only this will \n",
    "        # be one time step ahead of decoder input\n",
    "        for curr_timestep, char in enumerate(target_text):\n",
    "            dec_target_data[i, curr_timestep-1, target_char_idx[char]] = 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want the weights to be same for the different timesteps so for achieving that we do global declaration for the various components.\n",
    "\n",
    "Also we return the state information from the encoder network and use that information for the decoder network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ENCODER network\n",
    "# for taking input for the encoder network\n",
    "encoder_inputs = Input(shape=(None, encoder_unique_tokens))\n",
    "# we will LSTM units \n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "# we will save the activation and cell mem state information of encoder network\n",
    "# No need to save the outputs\n",
    "_, activation, cell_mem = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [activation, cell_mem]\n",
    "\n",
    "# for DECODER network\n",
    "# we will use the encoder state information as initial state for decoder network\n",
    "decoder_inputs = Input(shape = (None, decoder_unique_tokens))\n",
    "# we will save the state info of decoder network and use it \n",
    "# for making predictions later and return the output from decoder network units\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "# get the LSTM outputs\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# pass the LSTM output to a softmax layer\n",
    "decoder_dense = Dense(decoder_unique_tokens, activation = 'softmax')\n",
    "# get the final output from the softmax layer\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Model( [encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load any previously saved model\n",
    "model_path = r'models/fra_eng_wt.h5'\n",
    "if os.path.exists(model_path):\n",
    "    model.load_weights(r'models/fra_eng_wt.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None, 71)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None, 94)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 256), (None, 335872      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 256),  359424      input_4[0][0]                    \n",
      "                                                                 lstm_3[0][1]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 94)     24158       lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 719,454\n",
      "Trainable params: 719,454\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.5297 - val_loss: 0.6380\n",
      "Epoch 2/15\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.5067 - val_loss: 0.6144\n",
      "Epoch 3/15\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.4860 - val_loss: 0.5980\n",
      "Epoch 4/15\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.4702 - val_loss: 0.5827\n",
      "Epoch 5/15\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.4535 - val_loss: 0.5675\n",
      "Epoch 6/15\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.4397 - val_loss: 0.5555\n",
      "Epoch 7/15\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.4274 - val_loss: 0.5458\n",
      "Epoch 8/15\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.4156 - val_loss: 0.5390\n",
      "Epoch 9/15\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.4048 - val_loss: 0.5288\n",
      "Epoch 10/15\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.3949 - val_loss: 0.5188\n",
      "Epoch 11/15\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.3853 - val_loss: 0.5117\n",
      "Epoch 12/15\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.3765 - val_loss: 0.5059\n",
      "Epoch 13/15\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.3677 - val_loss: 0.5016\n",
      "Epoch 14/15\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3599 - val_loss: 0.4974\n",
      "Epoch 15/15\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.3517 - val_loss: 0.4888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUSANTA\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py:2344: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_3/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_3/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# start training the model\n",
    "model.fit( [enc_input_data, dec_input_data], dec_target_data,\n",
    "         batch_size = batch_size, epochs = num_epochs, \n",
    "         validation_split = 0.2)\n",
    "# save the model\n",
    "model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Sampling\n",
    "To see how good the model is working we sample outputs from it, so we are going to do that.\n",
    "\n",
    "Our Model can be divided into two parts: Encoder and Decoder. <br>\n",
    "1. We pass the text input through the encoder network to get the states.\n",
    "2. We use the encoder states as initial states for the decoder network.\n",
    "3. We feed start character '\\t' as the input for the first time step to the decoder network and then the predicted output is fed as input to the next time step.\n",
    "4. We do this till we get '\\n' or exceed max char length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the encoder part \n",
    "# this basically takes the encoder part of the training model as \n",
    "# we are saying that the input is encoder_inputs and as output we get encoder_outputs\n",
    "encoder_model = Model( encoder_inputs, encoder_states)\n",
    "\n",
    "# for the decoder part\n",
    "decoder_activation_state_input = Input(shape=(latent_dim,))\n",
    "decoder_mem_state_input = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_activation_state_input, decoder_mem_state_input]\n",
    "# using decoder lstm\n",
    "decoder_outputs, activation_state, mem_state = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [activation_state, mem_state]\n",
    "# the activations go through the softmax layer\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# model compilation\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Translations\n",
    "Now we will make translations for the input sequence. For that we first pass the input sequence through the encoder and then pass its state info to the decoder network and do decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_translation(input_text):\n",
    "    \n",
    "    # encoder state values\n",
    "    state_vals = encoder_model.predict(input_text)\n",
    "    \n",
    "    # make a target input consisting of start character '\\t'\n",
    "    dec_input_seq = np.zeros((1,1, decoder_unique_tokens), dtype = 'float32')\n",
    "    dec_input_seq[0, 0, target_char_idx['\\t']] = 1\n",
    "    \n",
    "    # now we start the translation process by sampling out the predictions\n",
    "    # each time sampling a single character\n",
    "    translated_text = ''\n",
    "    # decides whether to continue generating samples, becomes false\n",
    "    # on encountering '\\n' or when the the output sequence length exceeds max limit\n",
    "    run_loop = True\n",
    "    \n",
    "    while run_loop:\n",
    "        output_tokens, acti, mem = decoder_model.predict([dec_input_seq] + state_vals)\n",
    "        \n",
    "        # sample a char token\n",
    "        # since we get softmax prob. from the output layer, we pick the \n",
    "        # index with max prob.\n",
    "        sampled_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = target_idx_char[sampled_token_idx]\n",
    "        translated_text += sampled_token\n",
    "        \n",
    "        # check for loop condition\n",
    "        if len(translated_text)> Ty or sampled_token == '\\n':\n",
    "            run_loop = False\n",
    "        \n",
    "        # now update the decoder input for the next time step\n",
    "        dec_input_seq[0, 0, sampled_token_idx] = 1\n",
    "        \n",
    "        # update state values\n",
    "        state_vals = [acti, mem]\n",
    "        \n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soistttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(input_texts):\n",
    "    if j == 'Be nice.':\n",
    "        seq_index = i\n",
    "input_seq = enc_input_data[seq_index: seq_index + 1]\n",
    "decoded_sentence = do_translation(input_seq)\n",
    "print('-')\n",
    "print('Input sentence:', input_texts[seq_index])\n",
    "print('Decoded sentence:', decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Perrrrs aussssssssssssssssssssssssssssssssssssssssssssssssss\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Ressl-----.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Ressl-----.\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Quuelll llllllllllllllllllllllllllllllllllllllllllllllllllll\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Laissssssssssssssssssssst...................................\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Soitrrrrrrerrrnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Prerrrrs usassssssssssssssssssssssssssssssssssssssssssssssss\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrêttte ttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrêttte ttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrêttte ttttttttttttttttttttttttttttttttttttttttttttttttttt\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = enc_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = do_translation(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
